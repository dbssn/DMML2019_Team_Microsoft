{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "feature selection and model evaluation",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dbssn/DMML2019_Team_Microsoft/blob/master/feature_selection_and_model_evaluation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5HOkISoW4pQF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn import tree\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier,GradientBoostingClassifier,VotingClassifier,AdaBoostClassifier\n",
        "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
        "import xgboost as xgb\n",
        "import numpy as np\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from keras.models import Sequential\n",
        "from keras import layers\n",
        "from keras import optimizers\n",
        "import matplotlib.pyplot as plt\n",
        "import warnings\n",
        "from sklearn.linear_model import Lasso,Ridge\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from keras.wrappers import scikit_learn\n",
        "from sklearn import preprocessing\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.feature_selection import chi2,SelectKBest,mutual_info_classif,SelectFromModel\n",
        "\n",
        "warnings.filterwarnings(module='sklearn*', action='ignore', category=DeprecationWarning)\n",
        "\n",
        "df=pd.read_csv(\"\")\n",
        "y=df[\"\"]\n",
        "x=df.drop([\"\"],axis=1)\n",
        "\n",
        "#feature selection 1, univariate method  â†’ chi2 or mutual_info_classif\n",
        "x_new = SelectKBest(chi2, k=2).fit_transform(x, y)\n",
        "\n",
        "#feature selection 2, select from model including svc,logit,decision tree (try and error)\n",
        "clf = RandomForestClassifier(n_estimators=50)\n",
        "clf = clf.fit(x, y)\n",
        "model = SelectFromModel(clf, prefit=True)\n",
        "x_new = model.transform(x)\n",
        "           \n",
        "\n",
        "kf = KFold(n_splits=5, random_state=None, shuffle=True)\n",
        "\n",
        "#classification algorithm\n",
        "\n",
        "# I personally like to choose the best 3 classifier into vote from experiments. \n",
        "def vote(x,y):\n",
        "    clf1 = KNeighborsClassifier()\n",
        "    clf2 = LogisticRegression()\n",
        "    clf3 = SVC(probability=True)\n",
        "    eclf = VotingClassifier(estimators=[('1', clf1), ('2', clf2), ('3', clf3)])\n",
        "    scores = cross_val_score(eclf,x,y,cv=kf)\n",
        "    print(\"vote:\",round(scores.mean(),3))\n",
        "    out.append(round(scores.mean(),3))\n",
        "    \n",
        "def gbc(x,y):\n",
        "    clf = GradientBoostingClassifier()\n",
        "    scores = cross_val_score(clf,x,y,cv=kf)\n",
        "    print(\"gbc:\",round(scores.mean(),3))\n",
        "    out.append(round(scores.mean(),3))\n",
        "    \n",
        "def ada(x,y):\n",
        "    clf = AdaBoostClassifier()\n",
        "    scores = cross_val_score(clf,x,y,cv=kf)\n",
        "    print(\"ada:\",round(scores.mean(),3))\n",
        "    out.append(round(scores.mean(),3))\n",
        "    \n",
        "def qda(x,y):\n",
        "    clf=QuadraticDiscriminantAnalysis()\n",
        "    scores = cross_val_score(clf,x,y,cv=kf)\n",
        "    print(\"qda:\",round(scores.mean(),3))\n",
        "    out.append(round(scores.mean(),3))\n",
        "    \n",
        "def mlp(x,y):\n",
        "    mlp=MLPClassifier(hidden_layer_sizes=(200, ))\n",
        "    scores = cross_val_score(mlp,x,y,cv=kf)\n",
        "    print(\"mlp:\",round(scores.mean(),3))\n",
        "    out.append(round(scores.mean(),3))\n",
        "    \n",
        "def bayes(x,y):\n",
        "    clf = GaussianNB()\n",
        "    scores = cross_val_score(clf,x,y, cv=kf)\n",
        "    print(\"bayes:\",round(scores.mean(),3))\n",
        "    out.append(round(scores.mean(),3))\n",
        "    \n",
        "def knn(x,y):\n",
        "    neigh = KNeighborsClassifier()\n",
        "    scores = cross_val_score(neigh,x,y, cv=kf)\n",
        "    print(\"knn:\",round(scores.mean(),3))\n",
        "    out.append(round(scores.mean(),3))\n",
        "    \n",
        "def logistic(x,y):\n",
        "    classifier = LogisticRegression()\n",
        "    scores = cross_val_score(classifier,x,y, cv=kf)\n",
        "    print(\"logit:\",round(scores.mean(),3))\n",
        "    out.append(round(scores.mean(),3))\n",
        "    \n",
        "def forest(x,y):\n",
        "    forest = RandomForestClassifier()\n",
        "    scores = cross_val_score(forest,x,y, cv=kf)\n",
        "    print(\"forest:\",round(scores.mean(),3))\n",
        "    out.append(round(scores.mean(),3))\n",
        "    \n",
        "def xg(x,y):\n",
        "    x=np.array(x)\n",
        "    xgbc=xgb.XGBClassifier()\n",
        "    scores = cross_val_score(xgbc, x,y, cv=kf)\n",
        "    print(\"xgb:\",round(scores.mean(),3))\n",
        "    out.append(round(scores.mean(),3))\n",
        "    \n",
        "def tr(x,y):\n",
        "    tre = tree.DecisionTreeClassifier()\n",
        "    scores = cross_val_score(tre,x,y, cv=kf)\n",
        "    print(\"DecisionTree:\",round(scores.mean(),3))\n",
        "    out.append(round(scores.mean(),3))\n",
        "    \n",
        "def sv(x,y):\n",
        "    clf = SVC(gamma='auto')\n",
        "    scores = cross_val_score(clf,x,y, cv=kf)\n",
        "    print(\"svc:\",round(scores.mean(),3))\n",
        "    out.append(round(scores.mean(),3))\n",
        "    \n",
        "def pipe(x,y):\n",
        "    bayes(x,y)\n",
        "    tr(x,y)\n",
        "    knn(x,y)\n",
        "    xg(x,y)\n",
        "    forest(x,y)\n",
        "    gbc(x,y)\n",
        "    sv(x,y)\n",
        "    vote(x,y)\n",
        "    ada(x,y)\n",
        "    logistic(x,y)\n",
        "\n",
        "#To plot the results of Neural network\n",
        "def plot(history):\n",
        "    plt.style.use('ggplot')\n",
        "    acc = history.history['acc']\n",
        "    val_acc = history.history['val_acc']\n",
        "    loss = history.history['loss']\n",
        "    val_loss = history.history['val_loss']\n",
        "    x = range(1, len(acc) + 1)\n",
        "    plt.figure(figsize=(12, 5))\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(x, acc, 'b', label='Training acc')\n",
        "    plt.plot(x, val_acc, 'r', label='Validation acc')\n",
        "    plt.title('Training and validation accuracy')\n",
        "    plt.legend()\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(x, loss, 'b', label='Training loss')\n",
        "    plt.plot(x, val_loss, 'r', label='Validation loss')\n",
        "    plt.title('Training and validation loss')\n",
        "    plt.legend()  \n",
        "    \n",
        "# Neural network using Keras\n",
        "xtrain,xtest,ytrain,ytest = train_test_split(x,y,test_size=0.2)\n",
        "model = Sequential()\n",
        "a=optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999,amsgrad=False)\n",
        "model.add(layers.Dense(6, activation='relu'))\n",
        "model.add(layers.Dense(3, activation='relu'))\n",
        "model.add(layers.Dense(1, activation='sigmoid'))\n",
        "model.compile(loss='binary_crossentropy', optimizer=a, metrics=['accuracy'])\n",
        "out = model.fit(xtrain,ytrain,epochs=30,verbose=True,validation_data=(xtest, ytest),batch_size=5)\n",
        "print(model.summary())\n",
        "loss, accuracy = model.evaluate(xtrain, ytrain, verbose=False)\n",
        "print(\"Training Accuracy:  {:.3f}\".format(round(accuracy,2)))\n",
        "loss, accuracy = model.evaluate(xtest, ytest, verbose=False)\n",
        "print(\"Testing Accuracy:  {:.3f}\".format(round(accuracy,2)))\n",
        "plot(out)\n",
        "\n",
        "# Grid search for parameter tuning, the items in dict depending on model\n",
        "model=LogisticRegression()\n",
        "param_grid = dict()\n",
        "grid = GridSearchCV(estimator=model, param_grid=param_grid,cv=kf,scoring=\"accuracy\")\n",
        "grid_result = grid.fit(x,y)\n",
        "print(grid_result.best_params_)\n",
        "print(grid_result.best_score_)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}