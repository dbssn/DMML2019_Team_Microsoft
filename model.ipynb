{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "model",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dbssn/DMML2019_Team_Microsoft/blob/master/model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nr71aFtGIce-",
        "colab_type": "text"
      },
      "source": [
        "First, read the dataframe after cleaning."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7SEWyXyoDk4m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings(module='sklearn*', action='ignore', category=DeprecationWarning)\n",
        "df=pd.read_csv(\"\")\n",
        "y=df[\"\"]\n",
        "x=df.drop([\"\"],axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1u7PHVaSJAwL",
        "colab_type": "text"
      },
      "source": [
        "Then, do the feature selection to lower the dimensionality."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "moEeOdVuI85i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.feature_selection import chi2,SelectKBest,mutual_info_classif,SelectFromModel\n",
        "\n",
        "#feature selection 1, univariate method  â†’ chi2 or mutual_info_classif\n",
        "n=20\n",
        "x_select1 = SelectKBest(chi2, k=n).fit_transform(x, y)\n",
        "x_select2 = SelectKBest(mutual_info_classif, k=n).fit_transform(x, y)\n",
        "\n",
        "#feature selection 2, select from model including svc,logit,decision tree\n",
        "clf = RandomForestClassifier()\n",
        "clf = clf.fit(x, y)\n",
        "model = SelectFromModel(clf)\n",
        "x_select3 = model.transform(x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-w--UkmSMjtR",
        "colab_type": "text"
      },
      "source": [
        "Classifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JPRgY1xIMmG7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.svm import SVC\n",
        "from sklearn import tree\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier,GradientBoostingClassifier,VotingClassifier,AdaBoostClassifier\n",
        "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
        "import xgboost as xgb\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "\n",
        "\n",
        "kf = KFold(n_splits=5, random_state=None, shuffle=True)\n",
        "\n",
        "out=[]\n",
        "\n",
        "def gbc(x,y):\n",
        "    clf = GradientBoostingClassifier()\n",
        "    scores = cross_val_score(clf,x,y,cv=kf)\n",
        "    out.append(round(scores.mean(),3))\n",
        "    \n",
        "def ada(x,y):\n",
        "    clf = AdaBoostClassifier()\n",
        "    scores = cross_val_score(clf,x,y,cv=kf)\n",
        "    out.append(round(scores.mean(),3))\n",
        "    \n",
        "def qda(x,y):\n",
        "    clf=QuadraticDiscriminantAnalysis()\n",
        "    scores = cross_val_score(clf,x,y,cv=kf)\n",
        "    out.append(round(scores.mean(),3))\n",
        "    \n",
        "def mlp(x,y):\n",
        "    mlp=MLPClassifier(hidden_layer_sizes=(200, ))\n",
        "    scores = cross_val_score(mlp,x,y,cv=kf)\n",
        "    out.append(round(scores.mean(),3))\n",
        "    \n",
        "def bayes(x,y):\n",
        "    clf = MultinomialNB()\n",
        "    scores = cross_val_score(clf,x,y, cv=kf)\n",
        "    out.append(round(scores.mean(),3))\n",
        "    \n",
        "def knn(x,y):\n",
        "    neigh = KNeighborsClassifier()\n",
        "    scores = cross_val_score(neigh,x,y, cv=kf)\n",
        "    out.append(round(scores.mean(),3))\n",
        "    \n",
        "def logistic(x,y):\n",
        "    classifier = LogisticRegression()\n",
        "    scores = cross_val_score(classifier,x,y, cv=kf)\n",
        "    out.append(round(scores.mean(),3))\n",
        "    \n",
        "def forest(x,y):\n",
        "    forest = RandomForestClassifier()\n",
        "    scores = cross_val_score(forest,x,y, cv=kf)\n",
        "    out.append(round(scores.mean(),3))\n",
        "    \n",
        "def xg(x,y):\n",
        "    x=np.array(x)\n",
        "    xgbc=xgb.XGBClassifier()\n",
        "    scores = cross_val_score(xgbc, x,y, cv=kf)\n",
        "    out.append(round(scores.mean(),3))\n",
        "    \n",
        "def tr(x,y):\n",
        "    tre = tree.DecisionTreeClassifier()\n",
        "    scores = cross_val_score(tre,x,y, cv=kf)\n",
        "    out.append(round(scores.mean(),3))\n",
        "    \n",
        "def sv(x,y):\n",
        "    clf = SVC(gamma='auto')\n",
        "    scores = cross_val_score(clf,x,y, cv=kf)\n",
        "    out.append(round(scores.mean(),3))\n",
        "    \n",
        "def pipe(x,y):\n",
        "    gbc(x,y)\n",
        "    ada(x,y)\n",
        "    qda(x,y)\n",
        "    mlp(x,y)\n",
        "    bayes(x,y)\n",
        "    knn(x,y)\n",
        "    logistic(x,y)\n",
        "    forest(x,y)\n",
        "    xg(x,y)\n",
        "    tr(x,y)\n",
        "    sv(x,y)\n",
        "\n",
        "pipe(x,y)\n",
        "pipe(x_select1,y)\n",
        "pipe(x_select2,y)\n",
        "pipe(x_select3,y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RzLkAmZ3Q9az",
        "colab_type": "text"
      },
      "source": [
        "Visualize the accuracy rate"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KhXMyMmcQ9If",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.font_manager import FontProperties\n",
        "\n",
        "myfont = FontProperties(fname='D:/Programs/Lib/site-packages/matplotlib/mpl-data/fonts/ttf/msjh.ttc', size=40)\n",
        "xticks=[\"ada\",\"qda\",\"mlp\",\"bayes\",\"knn\",\"logistic\",\"forest\",\"xg\",\"tr\",\"sv\"]\n",
        "x_acc=out[:11]\n",
        "x_select1_acc=out[12:23]\n",
        "x_select2_acc=out[24:35]\n",
        "x_select3_acc=out[36:]\n",
        "plt.figure(figsize=(15,10),dpi=100,linewidth = 2)\n",
        "plt.plot(xticks,x_acc,'s-',color = 'r', label=\"x_acc\")\n",
        "plt.plot(xticks,x_select1,'o-',color = 'b', label=\"x_select1\")\n",
        "plt.plot(xticks,x_select2,'^-',color = 'g', label=\"x_select2\")\n",
        "plt.plot(xticks,x_select3,'*-',color = 'c', label=\"x_select3\")\n",
        "plt.title(\"Accuracy Comparison\"\", fontproperties=myfont, x=0.5, y=1.03)\n",
        "plt.xticks(fontsize=20)\n",
        "plt.yticks(fontsize=20)\n",
        "plt.xlabel(\"Classifier\", fontsize=30, labelpad = 15)\n",
        "plt.ylabel(\"Input\", fontsize=30, labelpad = 20)\n",
        "plt.legend(loc = \"best\", fontsize=20)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D6VSmz_bbsO4",
        "colab_type": "text"
      },
      "source": [
        "Choose the better input and classifiers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HdpT0oXlPUMt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# I personally like to choose the best 3 classifier into vote from experiments. \n",
        "def vote(x,y):\n",
        "    clf1 = \n",
        "    clf2 = \n",
        "    clf3 = \n",
        "    eclf = VotingClassifier(estimators=[('1', clf1), ('2', clf2), ('3', clf3)])\n",
        "    scores = cross_val_score(eclf,x,y,cv=kf)\n",
        "    out.append(round(scores.mean(),3))\n",
        "\n",
        "vote(,)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3OC7SP19eQXd",
        "colab_type": "text"
      },
      "source": [
        "Try the neural network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UW-iSGxUdijH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from keras.models import Sequential\n",
        "from keras import layers\n",
        "from keras import optimizers\n",
        "\n",
        "def plot(history):\n",
        "    plt.style.use('ggplot')\n",
        "    acc = history.history['acc']\n",
        "    val_acc = history.history['val_acc']\n",
        "    loss = history.history['loss']\n",
        "    val_loss = history.history['val_loss']\n",
        "    x = range(1, len(acc) + 1)\n",
        "    plt.figure(figsize=(12, 5))\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(x, acc, 'b', label='Training acc')\n",
        "    plt.plot(x, val_acc, 'r', label='Validation acc')\n",
        "    plt.title('Training and validation accuracy')\n",
        "    plt.legend()\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(x, loss, 'b', label='Training loss')\n",
        "    plt.plot(x, val_loss, 'r', label='Validation loss')\n",
        "    plt.title('Training and validation loss')\n",
        "    plt.legend()\n",
        "\n",
        "xtrain,xtest,ytrain,ytest = train_test_split(x,y,test_size=0.2)\n",
        "model = Sequential()\n",
        "opt=optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999,amsgrad=False)\n",
        "model.add(layers.Dense(6, activation='relu'))\n",
        "model.add(layers.Dense(3, activation='relu'))\n",
        "model.add(layers.Dense(1, activation='sigmoid'))\n",
        "model.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
        "out = model.fit(xtrain,ytrain,epochs=30,verbose=True,validation_data=(xtest, ytest),batch_size=5)\n",
        "print(model.summary())\n",
        "loss, accuracy = model.evaluate(xtrain, ytrain, verbose=False)\n",
        "print(\"Training Accuracy:  {:.3f}\".format(round(accuracy,2)))\n",
        "loss, accuracy = model.evaluate(xtest, ytest, verbose=False)\n",
        "print(\"Testing Accuracy:  {:.3f}\".format(round(accuracy,2)))\n",
        "plot(out)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LBgmdW4Leynu",
        "colab_type": "text"
      },
      "source": [
        "Using gridsearch to tune the hyper parameter"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EOlIh1p4eiow",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "from keras.wrappers import scikit_learn\n",
        "\n",
        "model=\n",
        "param_grid = dict()\n",
        "grid = GridSearchCV(estimator=model, param_grid=param_grid,cv=kf,scoring=\"accuracy\")\n",
        "grid_result = grid.fit(x,y)\n",
        "print(grid_result.best_params_)\n",
        "print(grid_result.best_score_)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dSNdWfI0koSp",
        "colab_type": "text"
      },
      "source": [
        "Show the confusion matrix and other score"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L_IZW09mi8jz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import plot_confusion_matrix\n",
        "from sklearn.metrics import plot_precision_recall_curve\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import recall_score\n",
        "from sklearn.metrics import precision_score\n",
        "\n",
        "classifier = svm.SVC(kernel='linear', C=0.01).fit(X_train, y_train)\n",
        "\n",
        "con = plot_confusion_matrix(classifier, x_test, y_test,display_labels=[\"pass\",\"fail\"],cmap=plt.cm.Blues,normalize=normalize)\n",
        "con.ax_.set_title(\"Confusion Matrix\")\n",
        "plt.show()\n",
        "\n",
        "disp = plot_precision_recall_curve(classifier, X_test, y_test)\n",
        "disp.ax_.set_title(\"2-class Precision-Recall curve\")\n",
        "\n",
        "f1_score(y_true, y_pred)\n",
        "recall_score(y_true, y_pred)\n",
        "precision_score(y_true, y_pred)\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}